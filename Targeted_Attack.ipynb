{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Targeted_Attack.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQUHXAFXr2Xo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "527b1c17-2ab8-4686-a979-3e56e0593488"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ok2IAvar2X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)    \n",
        "\n",
        "model_dnn_2 = nn.Sequential(Flatten(), nn.Linear(784,200), nn.ReLU(), \n",
        "                            nn.Linear(200,10)).to(device)\n",
        "\n",
        "model_dnn_4 = nn.Sequential(Flatten(), nn.Linear(784,200), nn.ReLU(), \n",
        "                            nn.Linear(200,100), nn.ReLU(),\n",
        "                            nn.Linear(100,100), nn.ReLU(),\n",
        "                            nn.Linear(100,10)).to(device)\n",
        "\n",
        "model_cnn = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
        "                          nn.Conv2d(32, 32, 3, padding=1, stride=2), nn.ReLU(),\n",
        "                          nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "                          nn.Conv2d(64, 64, 3, padding=1, stride=2), nn.ReLU(),\n",
        "                          Flatten(),\n",
        "                          nn.Linear(7*7*64, 100), nn.ReLU(),\n",
        "                          nn.Linear(100, 10)).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8HsPo2kr2X3",
        "colab_type": "text"
      },
      "source": [
        "train.. trains the models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNqKbo68r2X4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "mnist_train = datasets.MNIST(\"../data\", train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(\"../data\", train=False, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYuiwXFXr2X7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch(loader, model, opt=None):\n",
        "    total_loss, total_err = 0.,0.\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(device), y.to(device)\n",
        "        yp = model(X)\n",
        "        loss = nn.CrossEntropyLoss()(yp,y)\n",
        "        if opt:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        \n",
        "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
        "        total_loss += loss.item() * X.shape[0]\n",
        "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeAE0drsr2X9",
        "colab_type": "code",
        "outputId": "cad6fa04-400e-4c2e-8986-64c78cb465a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "opt = optim.SGD(model_dnn_2.parameters(), lr=1e-1)\n",
        "for _ in range(10):\n",
        "    train_err, train_loss = epoch(train_loader, model_dnn_2, opt)\n",
        "    test_err, test_loss = epoch(test_loader, model_dnn_2)\n",
        "    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.131833\t0.510995\t0.081200\t0.286766\n",
            "0.075933\t0.264755\t0.062600\t0.220393\n",
            "0.059733\t0.210222\t0.053800\t0.182406\n",
            "0.049333\t0.173824\t0.047700\t0.158933\n",
            "0.042233\t0.148129\t0.038800\t0.136713\n",
            "0.036483\t0.128240\t0.036700\t0.123244\n",
            "0.032017\t0.113127\t0.033400\t0.112570\n",
            "0.028550\t0.100832\t0.030500\t0.102933\n",
            "0.025017\t0.090675\t0.029900\t0.100128\n",
            "0.022583\t0.082377\t0.029700\t0.099003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQZUNb_Dr2YB",
        "colab_type": "code",
        "outputId": "e85e1d64-a0b8-4bfb-b360-7bcb53d48ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "opt = optim.SGD(model_dnn_4.parameters(), lr=1e-1)\n",
        "for _ in range(10):\n",
        "    train_err, train_loss = epoch(train_loader, model_dnn_4, opt)\n",
        "    test_err, test_loss = epoch(test_loader, model_dnn_4)\n",
        "    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.230483\t0.760702\t0.085500\t0.281864\n",
            "0.065100\t0.220869\t0.045500\t0.151801\n",
            "0.042217\t0.143030\t0.035800\t0.117922\n",
            "0.031800\t0.106133\t0.032200\t0.105105\n",
            "0.025883\t0.084971\t0.027300\t0.086052\n",
            "0.020483\t0.068996\t0.028600\t0.091057\n",
            "0.017200\t0.057379\t0.024700\t0.078578\n",
            "0.014883\t0.049171\t0.024600\t0.077526\n",
            "0.012600\t0.041440\t0.026500\t0.085066\n",
            "0.010817\t0.035355\t0.022400\t0.076369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je5-RAXVr2YD",
        "colab_type": "code",
        "outputId": "f1342db1-f23a-49e8-ae77-1e5678660a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "opt = optim.SGD(model_cnn.parameters(), lr=1e-1)\n",
        "for t in range(10):\n",
        "    train_err, train_loss = epoch(train_loader, model_cnn, opt)\n",
        "    test_err, test_loss = epoch(test_loader, model_cnn)\n",
        "    if t == 4:\n",
        "        for param_group in opt.param_groups:\n",
        "            param_group[\"lr\"] = 1e-2\n",
        "    print(*(\"{:.6f}\".format(i) for i in (train_err, train_loss, test_err, test_loss)), sep=\"\\t\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.235417\t0.674366\t0.035900\t0.111383\n",
            "0.027317\t0.089329\t0.021200\t0.068788\n",
            "0.017850\t0.057688\t0.016600\t0.051576\n",
            "0.013917\t0.043483\t0.014900\t0.045710\n",
            "0.010217\t0.032973\t0.015400\t0.049015\n",
            "0.004683\t0.015756\t0.012400\t0.038307\n",
            "0.003467\t0.012394\t0.011700\t0.038140\n",
            "0.003083\t0.010924\t0.012000\t0.038105\n",
            "0.002600\t0.009754\t0.012000\t0.039070\n",
            "0.002367\t0.008930\t0.012600\t0.039601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKNZshNZr2YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model_dnn_2.state_dict(), \"model_dnn_2.pt\")\n",
        "torch.save(model_dnn_4.state_dict(), \"model_dnn_4.pt\")\n",
        "torch.save(model_cnn.state_dict(), \"model_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHclPx2Br2ZI",
        "colab_type": "text"
      },
      "source": [
        "### Targeted attacks\n",
        "\n",
        "What we have considered so far are \"untargeted\" attacks, meaning they effectively try to change the label to _any_ alternative, rather than change it to a particular alternative.  As a different task, which we saw briefly in the introduction, we can change the attack to try to convert the prediction to a particular alernative.  This is a task known as a \"targetted attack\", and it can be achieved using the same strategy overall strategy as we did previously.  However, in this case the only difference is that instead of maximizing the loss of the true label, we maximize the loss of the loss of the true label and also minimize the loss for the alternative label.  This is equivalent to solving the inner optimization problem\n",
        "\\begin{equation}\n",
        "\\maximize_{\\|\\delta\\| \\leq \\epsilon} \\left ( \\ell(h_\\theta(x + \\delta), y) - \\ell(h_\\theta(x + \\delta), y_{\\mathrm{targ}}) \\right ) \\equiv \\maximize_{\\|\\delta\\| \\leq \\epsilon} \\left ( h_\\theta(x + \\delta)_{y_{\\mathrm{targ}}} - h_\\theta(x + \\delta)_y \\right )\n",
        "\\end{equation}\n",
        "Let's see what this looks like, using a PGD attack (without randomized restarts).  Note that in order to achieve our targetted class in most of these cases on MNIST, we use a slightly larger perturbation region, $\\epsilon=0.2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y57oFPMtr2ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pgd_linf_targ(model, X, y, epsilon, alpha, num_iter, y_targ):\n",
        "    \"\"\" Construct targeted adversarial examples on the examples X\"\"\"\n",
        "    delta = torch.zeros_like(X, requires_grad=True)\n",
        "    for t in range(num_iter):\n",
        "        yp = model(X + delta)\n",
        "        loss = (yp[:,y_targ] - yp.gather(1,y[:,None])[:,0]).sum()\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr4K1b4Qr2ZK",
        "colab_type": "text"
      },
      "source": [
        "Let's look at trying to make the class label all equation to 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-7LBWLVr2ZL",
        "colab_type": "code",
        "outputId": "6eda5f0a-2c69-49de-d592-f0bf2bfa784f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "delta = pgd_linf_targ(model_cnn, X, y, epsilon=0.2, alpha=1e-2, num_iter=40, y_targ=2)\n",
        "yp = model_cnn(X + delta)\n",
        "plot_images(X+delta, y, yp, 3, 6)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-260f0af9b1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpgd_linf_targ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_targ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0myp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJsNElnur2ZN",
        "colab_type": "text"
      },
      "source": [
        "This looks pretty good: albeit with a slightly larger $\\epsilon$, we can fool the classifier into predicting that all the examples are class 2 (note that the actual 2 is unchanged, because the loss function in this case is always exactly zero).  Let's try using a target class of 0 instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp7FwgPjr2ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delta = pgd_linf_targ(model_cnn, X, y, epsilon=0.2, alpha=1e-2, num_iter=40, y_targ=0)\n",
        "yp = model_cnn(X + delta)\n",
        "plot_images(X+delta, y, yp, 3, 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLGetKfdr2ZQ",
        "colab_type": "text"
      },
      "source": [
        "While we are able to fool the classifier for all the non-zero digits, it's worth pointing out that we don't actually achieve the target class here in all cases.  This is because the optimization objective we are maximizing is the class logit for the zero minus the class logit for the true class.  But we don't actually care what happens to the other classes, and in some cases, the best way to make the class 0 logit high is to make another class logit even higher.  We can get around this by modifying our objective to maximize the target class logit and minimize _all_ the other logits, i.e.,\n",
        "\\begin{equation}\n",
        "\\maximize_{\\|\\delta\\| \\leq \\epsilon} \\left ( h_\\theta(x + \\delta)_{y_{\\mathrm{targ}}} - \\sum_{y' \\neq y_{\\mathrm{targ}}} h_\\theta(x + \\delta)_{y'} \\right )\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3T8IAmnr2ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pgd_linf_targ2(model, X, y, epsilon, alpha, num_iter, y_targ):\n",
        "    \"\"\" Construct targeted adversarial examples on the examples X\"\"\"\n",
        "    delta = torch.zeros_like(X, requires_grad=True)\n",
        "    for t in range(num_iter):\n",
        "        yp = model(X + delta)\n",
        "        loss = 2*yp[:,y_targ].sum() - yp.sum()\n",
        "        loss.backward()\n",
        "        delta.data = (delta + alpha*delta.grad.detach().sign()).clamp(-epsilon,epsilon)\n",
        "        delta.grad.zero_()\n",
        "    return delta.detach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neQB3VrAr2ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delta = pgd_linf_targ(model_cnn, X, y, epsilon=0.2, alpha=1e-2, num_iter=40, y_targ=0)\n",
        "yp = model_cnn(X + delta)\n",
        "plot_images(X+delta, y, yp, 3, 6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP3Mrp0xr2ZT",
        "colab_type": "text"
      },
      "source": [
        "This is a more difficult objective than the previous one, so we aren't able to fool the classifier as much.  But when we _do_ fool the classifier, it more consistently (even if still not perfectly) able to predict the target class."
      ]
    }
  ]
}